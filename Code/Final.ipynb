{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words-Meet-Bags-of-Popcorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMOPRTING DATA FOR TRAINING AND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files \n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0,delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0,delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews,and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the punkt tokenizer for sentence splitting and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "nltk.download()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to split a review into parsed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:273: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.5/dist-packages/bs4/__init__.py:336: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just to be done once, after this built model can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-10 08:11:08,857 : INFO : collecting all words and their counts\n",
      "2018-10-10 08:11:08,859 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-10-10 08:11:08,928 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2018-10-10 08:11:09,002 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-10 08:11:09,080 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n",
      "2018-10-10 08:11:09,158 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2018-10-10 08:11:09,236 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
      "2018-10-10 08:11:09,315 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2018-10-10 08:11:09,397 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2018-10-10 08:11:09,478 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2018-10-10 08:11:09,558 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2018-10-10 08:11:09,634 : INFO : PROGRESS: at sentence #100000, processed 2226967 words, keeping 50207 word types\n",
      "2018-10-10 08:11:09,713 : INFO : PROGRESS: at sentence #110000, processed 2446581 words, keeping 52081 word types\n",
      "2018-10-10 08:11:09,784 : INFO : PROGRESS: at sentence #120000, processed 2668776 words, keeping 54119 word types\n",
      "2018-10-10 08:11:09,856 : INFO : PROGRESS: at sentence #130000, processed 2894304 words, keeping 55847 word types\n",
      "2018-10-10 08:11:09,924 : INFO : PROGRESS: at sentence #140000, processed 3107006 words, keeping 57346 word types\n",
      "2018-10-10 08:11:09,999 : INFO : PROGRESS: at sentence #150000, processed 3332628 words, keeping 59055 word types\n",
      "2018-10-10 08:11:10,081 : INFO : PROGRESS: at sentence #160000, processed 3555316 words, keeping 60617 word types\n",
      "2018-10-10 08:11:10,158 : INFO : PROGRESS: at sentence #170000, processed 3778656 words, keeping 62077 word types\n",
      "2018-10-10 08:11:10,234 : INFO : PROGRESS: at sentence #180000, processed 3999237 words, keeping 63496 word types\n",
      "2018-10-10 08:11:10,314 : INFO : PROGRESS: at sentence #190000, processed 4224450 words, keeping 64794 word types\n",
      "2018-10-10 08:11:10,397 : INFO : PROGRESS: at sentence #200000, processed 4448604 words, keeping 66087 word types\n",
      "2018-10-10 08:11:10,479 : INFO : PROGRESS: at sentence #210000, processed 4669968 words, keeping 67390 word types\n",
      "2018-10-10 08:11:10,560 : INFO : PROGRESS: at sentence #220000, processed 4894969 words, keeping 68697 word types\n",
      "2018-10-10 08:11:10,640 : INFO : PROGRESS: at sentence #230000, processed 5117546 words, keeping 69958 word types\n",
      "2018-10-10 08:11:10,717 : INFO : PROGRESS: at sentence #240000, processed 5345051 words, keeping 71167 word types\n",
      "2018-10-10 08:11:10,790 : INFO : PROGRESS: at sentence #250000, processed 5559166 words, keeping 72351 word types\n",
      "2018-10-10 08:11:10,863 : INFO : PROGRESS: at sentence #260000, processed 5779147 words, keeping 73478 word types\n",
      "2018-10-10 08:11:10,935 : INFO : PROGRESS: at sentence #270000, processed 6000436 words, keeping 74767 word types\n",
      "2018-10-10 08:11:11,018 : INFO : PROGRESS: at sentence #280000, processed 6226315 words, keeping 76369 word types\n",
      "2018-10-10 08:11:11,100 : INFO : PROGRESS: at sentence #290000, processed 6449475 words, keeping 77839 word types\n",
      "2018-10-10 08:11:11,180 : INFO : PROGRESS: at sentence #300000, processed 6674078 words, keeping 79171 word types\n",
      "2018-10-10 08:11:11,263 : INFO : PROGRESS: at sentence #310000, processed 6899392 words, keeping 80480 word types\n",
      "2018-10-10 08:11:11,344 : INFO : PROGRESS: at sentence #320000, processed 7124279 words, keeping 81808 word types\n",
      "2018-10-10 08:11:11,430 : INFO : PROGRESS: at sentence #330000, processed 7346022 words, keeping 83030 word types\n",
      "2018-10-10 08:11:11,519 : INFO : PROGRESS: at sentence #340000, processed 7575534 words, keeping 84280 word types\n",
      "2018-10-10 08:11:11,601 : INFO : PROGRESS: at sentence #350000, processed 7798804 words, keeping 85425 word types\n",
      "2018-10-10 08:11:11,682 : INFO : PROGRESS: at sentence #360000, processed 8019467 words, keeping 86596 word types\n",
      "2018-10-10 08:11:11,774 : INFO : PROGRESS: at sentence #370000, processed 8246659 words, keeping 87708 word types\n",
      "2018-10-10 08:11:11,854 : INFO : PROGRESS: at sentence #380000, processed 8471806 words, keeping 88878 word types\n",
      "2018-10-10 08:11:11,937 : INFO : PROGRESS: at sentence #390000, processed 8701556 words, keeping 89907 word types\n",
      "2018-10-10 08:11:12,020 : INFO : PROGRESS: at sentence #400000, processed 8924505 words, keeping 90916 word types\n",
      "2018-10-10 08:11:12,101 : INFO : PROGRESS: at sentence #410000, processed 9145855 words, keeping 91880 word types\n",
      "2018-10-10 08:11:12,183 : INFO : PROGRESS: at sentence #420000, processed 9366935 words, keeping 92912 word types\n",
      "2018-10-10 08:11:12,269 : INFO : PROGRESS: at sentence #430000, processed 9594472 words, keeping 93932 word types\n",
      "2018-10-10 08:11:12,351 : INFO : PROGRESS: at sentence #440000, processed 9821225 words, keeping 94906 word types\n",
      "2018-10-10 08:11:12,434 : INFO : PROGRESS: at sentence #450000, processed 10044987 words, keeping 96036 word types\n",
      "2018-10-10 08:11:12,520 : INFO : PROGRESS: at sentence #460000, processed 10277747 words, keeping 97088 word types\n",
      "2018-10-10 08:11:12,604 : INFO : PROGRESS: at sentence #470000, processed 10505672 words, keeping 97933 word types\n",
      "2018-10-10 08:11:12,685 : INFO : PROGRESS: at sentence #480000, processed 10726056 words, keeping 98862 word types\n",
      "2018-10-10 08:11:12,773 : INFO : PROGRESS: at sentence #490000, processed 10952800 words, keeping 99871 word types\n",
      "2018-10-10 08:11:12,857 : INFO : PROGRESS: at sentence #500000, processed 11174456 words, keeping 100765 word types\n",
      "2018-10-10 08:11:12,943 : INFO : PROGRESS: at sentence #510000, processed 11399731 words, keeping 101699 word types\n",
      "2018-10-10 08:11:13,027 : INFO : PROGRESS: at sentence #520000, processed 11623082 words, keeping 102598 word types\n",
      "2018-10-10 08:11:13,110 : INFO : PROGRESS: at sentence #530000, processed 11847480 words, keeping 103400 word types\n",
      "2018-10-10 08:11:13,192 : INFO : PROGRESS: at sentence #540000, processed 12072095 words, keeping 104265 word types\n",
      "2018-10-10 08:11:13,271 : INFO : PROGRESS: at sentence #550000, processed 12297646 words, keeping 105133 word types\n",
      "2018-10-10 08:11:13,349 : INFO : PROGRESS: at sentence #560000, processed 12518936 words, keeping 105997 word types\n",
      "2018-10-10 08:11:13,431 : INFO : PROGRESS: at sentence #570000, processed 12748083 words, keeping 106787 word types\n",
      "2018-10-10 08:11:13,512 : INFO : PROGRESS: at sentence #580000, processed 12969579 words, keeping 107665 word types\n",
      "2018-10-10 08:11:13,595 : INFO : PROGRESS: at sentence #590000, processed 13195104 words, keeping 108501 word types\n",
      "2018-10-10 08:11:13,676 : INFO : PROGRESS: at sentence #600000, processed 13417302 words, keeping 109218 word types\n",
      "2018-10-10 08:11:13,757 : INFO : PROGRESS: at sentence #610000, processed 13638325 words, keeping 110092 word types\n",
      "2018-10-10 08:11:13,841 : INFO : PROGRESS: at sentence #620000, processed 13864650 words, keeping 110837 word types\n",
      "2018-10-10 08:11:13,927 : INFO : PROGRESS: at sentence #630000, processed 14088936 words, keeping 111610 word types\n",
      "2018-10-10 08:11:14,007 : INFO : PROGRESS: at sentence #640000, processed 14309719 words, keeping 112416 word types\n",
      "2018-10-10 08:11:14,090 : INFO : PROGRESS: at sentence #650000, processed 14535475 words, keeping 113196 word types\n",
      "2018-10-10 08:11:14,171 : INFO : PROGRESS: at sentence #660000, processed 14758265 words, keeping 113945 word types\n",
      "2018-10-10 08:11:14,249 : INFO : PROGRESS: at sentence #670000, processed 14981658 words, keeping 114643 word types\n",
      "2018-10-10 08:11:14,330 : INFO : PROGRESS: at sentence #680000, processed 15206490 words, keeping 115354 word types\n",
      "2018-10-10 08:11:14,410 : INFO : PROGRESS: at sentence #690000, processed 15428683 words, keeping 116131 word types\n",
      "2018-10-10 08:11:14,493 : INFO : PROGRESS: at sentence #700000, processed 15657389 words, keeping 116943 word types\n",
      "2018-10-10 08:11:14,574 : INFO : PROGRESS: at sentence #710000, processed 15880378 words, keeping 117596 word types\n",
      "2018-10-10 08:11:14,659 : INFO : PROGRESS: at sentence #720000, processed 16105665 words, keeping 118221 word types\n",
      "2018-10-10 08:11:14,747 : INFO : PROGRESS: at sentence #730000, processed 16332046 words, keeping 118954 word types\n",
      "2018-10-10 08:11:14,828 : INFO : PROGRESS: at sentence #740000, processed 16553079 words, keeping 119668 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-10 08:11:14,911 : INFO : PROGRESS: at sentence #750000, processed 16771406 words, keeping 120295 word types\n",
      "2018-10-10 08:11:14,991 : INFO : PROGRESS: at sentence #760000, processed 16990810 words, keeping 120930 word types\n",
      "2018-10-10 08:11:15,080 : INFO : PROGRESS: at sentence #770000, processed 17217947 words, keeping 121703 word types\n",
      "2018-10-10 08:11:15,164 : INFO : PROGRESS: at sentence #780000, processed 17448093 words, keeping 122402 word types\n",
      "2018-10-10 08:11:15,246 : INFO : PROGRESS: at sentence #790000, processed 17675169 words, keeping 123066 word types\n",
      "2018-10-10 08:11:15,294 : INFO : collected 123504 word types from a corpus of 17798270 raw words and 795538 sentences\n",
      "2018-10-10 08:11:15,296 : INFO : Loading a fresh vocabulary\n",
      "2018-10-10 08:11:15,440 : INFO : effective_min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
      "2018-10-10 08:11:15,441 : INFO : effective_min_count=40 leaves 17239125 word corpus (96% of original 17798270, drops 559145)\n",
      "2018-10-10 08:11:15,512 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2018-10-10 08:11:15,524 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-10-10 08:11:15,525 : INFO : downsampling leaves estimated 12749798 word corpus (74.0% of prior 17239125)\n",
      "2018-10-10 08:11:15,634 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2018-10-10 08:11:15,636 : INFO : resetting layer weights\n",
      "2018-10-10 08:11:15,881 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-10-10 08:11:16,899 : INFO : EPOCH 1 - PROGRESS: at 4.27% examples, 538695 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:11:17,900 : INFO : EPOCH 1 - PROGRESS: at 8.78% examples, 554864 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 08:11:18,908 : INFO : EPOCH 1 - PROGRESS: at 13.37% examples, 561951 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 08:11:19,910 : INFO : EPOCH 1 - PROGRESS: at 17.96% examples, 565927 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:20,921 : INFO : EPOCH 1 - PROGRESS: at 22.53% examples, 567377 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:21,929 : INFO : EPOCH 1 - PROGRESS: at 27.25% examples, 572031 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:11:22,934 : INFO : EPOCH 1 - PROGRESS: at 32.07% examples, 576664 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:23,937 : INFO : EPOCH 1 - PROGRESS: at 36.73% examples, 578492 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:24,939 : INFO : EPOCH 1 - PROGRESS: at 41.49% examples, 581708 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:25,952 : INFO : EPOCH 1 - PROGRESS: at 46.40% examples, 585678 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:26,961 : INFO : EPOCH 1 - PROGRESS: at 51.36% examples, 589823 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:27,962 : INFO : EPOCH 1 - PROGRESS: at 56.21% examples, 592439 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:28,973 : INFO : EPOCH 1 - PROGRESS: at 61.08% examples, 594751 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:29,975 : INFO : EPOCH 1 - PROGRESS: at 65.95% examples, 596609 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:11:30,992 : INFO : EPOCH 1 - PROGRESS: at 70.82% examples, 597623 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:11:31,992 : INFO : EPOCH 1 - PROGRESS: at 76.16% examples, 602691 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:33,000 : INFO : EPOCH 1 - PROGRESS: at 81.43% examples, 606469 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:34,000 : INFO : EPOCH 1 - PROGRESS: at 86.25% examples, 606959 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:35,029 : INFO : EPOCH 1 - PROGRESS: at 91.04% examples, 606448 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:11:36,041 : INFO : EPOCH 1 - PROGRESS: at 96.08% examples, 607581 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:36,788 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-10 08:11:36,797 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-10 08:11:36,825 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-10 08:11:36,831 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-10 08:11:36,832 : INFO : EPOCH - 1 : training on 17798270 raw words (12750572 effective words) took 20.9s, 608873 effective words/s\n",
      "2018-10-10 08:11:37,848 : INFO : EPOCH 2 - PROGRESS: at 4.75% examples, 607792 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:38,861 : INFO : EPOCH 2 - PROGRESS: at 10.09% examples, 635488 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:39,875 : INFO : EPOCH 2 - PROGRESS: at 15.01% examples, 628434 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:11:40,889 : INFO : EPOCH 2 - PROGRESS: at 20.05% examples, 628247 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:41,889 : INFO : EPOCH 2 - PROGRESS: at 26.07% examples, 655443 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:42,897 : INFO : EPOCH 2 - PROGRESS: at 31.96% examples, 669066 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 08:11:43,915 : INFO : EPOCH 2 - PROGRESS: at 36.79% examples, 659594 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:44,928 : INFO : EPOCH 2 - PROGRESS: at 41.81% examples, 656609 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:45,939 : INFO : EPOCH 2 - PROGRESS: at 46.72% examples, 652798 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:46,958 : INFO : EPOCH 2 - PROGRESS: at 51.65% examples, 649286 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:47,963 : INFO : EPOCH 2 - PROGRESS: at 56.49% examples, 646482 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:11:48,977 : INFO : EPOCH 2 - PROGRESS: at 61.36% examples, 644352 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:49,996 : INFO : EPOCH 2 - PROGRESS: at 66.28% examples, 642325 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:11:51,001 : INFO : EPOCH 2 - PROGRESS: at 71.10% examples, 640142 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:52,029 : INFO : EPOCH 2 - PROGRESS: at 75.99% examples, 637836 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:11:53,046 : INFO : EPOCH 2 - PROGRESS: at 81.04% examples, 637514 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:54,053 : INFO : EPOCH 2 - PROGRESS: at 86.08% examples, 637686 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:55,073 : INFO : EPOCH 2 - PROGRESS: at 90.98% examples, 636517 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:11:56,074 : INFO : EPOCH 2 - PROGRESS: at 95.91% examples, 635712 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 08:11:56,893 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-10 08:11:56,894 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-10 08:11:56,922 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-10 08:11:56,926 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-10 08:11:56,928 : INFO : EPOCH - 2 : training on 17798270 raw words (12751878 effective words) took 20.1s, 635045 effective words/s\n",
      "2018-10-10 08:11:57,955 : INFO : EPOCH 3 - PROGRESS: at 4.81% examples, 604401 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:58,961 : INFO : EPOCH 3 - PROGRESS: at 9.81% examples, 614969 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:11:59,988 : INFO : EPOCH 3 - PROGRESS: at 14.89% examples, 618894 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:00,996 : INFO : EPOCH 3 - PROGRESS: at 19.76% examples, 616494 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:02,016 : INFO : EPOCH 3 - PROGRESS: at 24.61% examples, 613913 words/s, in_qsize 8, out_qsize 1\n",
      "2018-10-10 08:12:03,042 : INFO : EPOCH 3 - PROGRESS: at 29.42% examples, 611431 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:04,043 : INFO : EPOCH 3 - PROGRESS: at 34.39% examples, 612701 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:05,051 : INFO : EPOCH 3 - PROGRESS: at 39.07% examples, 610721 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:06,052 : INFO : EPOCH 3 - PROGRESS: at 43.92% examples, 611948 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:07,056 : INFO : EPOCH 3 - PROGRESS: at 48.82% examples, 613474 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:08,077 : INFO : EPOCH 3 - PROGRESS: at 53.77% examples, 613754 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-10 08:12:09,082 : INFO : EPOCH 3 - PROGRESS: at 58.57% examples, 614152 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:10,092 : INFO : EPOCH 3 - PROGRESS: at 63.39% examples, 613740 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:11,109 : INFO : EPOCH 3 - PROGRESS: at 68.26% examples, 613686 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:12,121 : INFO : EPOCH 3 - PROGRESS: at 73.00% examples, 612831 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:13,127 : INFO : EPOCH 3 - PROGRESS: at 77.83% examples, 612725 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:14,139 : INFO : EPOCH 3 - PROGRESS: at 82.77% examples, 613241 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:15,166 : INFO : EPOCH 3 - PROGRESS: at 87.74% examples, 613640 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:16,168 : INFO : EPOCH 3 - PROGRESS: at 92.56% examples, 613669 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:17,169 : INFO : EPOCH 3 - PROGRESS: at 97.46% examples, 614069 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:17,668 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-10 08:12:17,678 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-10 08:12:17,704 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-10 08:12:17,710 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-10 08:12:17,711 : INFO : EPOCH - 3 : training on 17798270 raw words (12752692 effective words) took 20.8s, 613886 effective words/s\n",
      "2018-10-10 08:12:18,720 : INFO : EPOCH 4 - PROGRESS: at 4.70% examples, 601160 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 08:12:19,721 : INFO : EPOCH 4 - PROGRESS: at 9.62% examples, 611145 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:20,748 : INFO : EPOCH 4 - PROGRESS: at 14.67% examples, 614232 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:21,749 : INFO : EPOCH 4 - PROGRESS: at 19.60% examples, 615906 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:22,759 : INFO : EPOCH 4 - PROGRESS: at 24.44% examples, 614690 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:23,760 : INFO : EPOCH 4 - PROGRESS: at 29.36% examples, 616953 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:24,762 : INFO : EPOCH 4 - PROGRESS: at 34.44% examples, 619436 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 08:12:25,784 : INFO : EPOCH 4 - PROGRESS: at 39.54% examples, 621655 words/s, in_qsize 8, out_qsize 1\n",
      "2018-10-10 08:12:26,788 : INFO : EPOCH 4 - PROGRESS: at 44.44% examples, 622265 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 08:12:27,789 : INFO : EPOCH 4 - PROGRESS: at 49.71% examples, 627904 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:28,793 : INFO : EPOCH 4 - PROGRESS: at 55.77% examples, 640760 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:29,799 : INFO : EPOCH 4 - PROGRESS: at 61.92% examples, 653023 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:30,823 : INFO : EPOCH 4 - PROGRESS: at 66.74% examples, 648985 words/s, in_qsize 8, out_qsize 1\n",
      "2018-10-10 08:12:31,830 : INFO : EPOCH 4 - PROGRESS: at 71.59% examples, 646734 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:32,856 : INFO : EPOCH 4 - PROGRESS: at 76.55% examples, 644484 words/s, in_qsize 8, out_qsize 1\n",
      "2018-10-10 08:12:33,862 : INFO : EPOCH 4 - PROGRESS: at 81.26% examples, 641582 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:34,866 : INFO : EPOCH 4 - PROGRESS: at 86.14% examples, 640309 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 08:12:35,885 : INFO : EPOCH 4 - PROGRESS: at 90.88% examples, 637843 words/s, in_qsize 5, out_qsize 2\n",
      "2018-10-10 08:12:36,890 : INFO : EPOCH 4 - PROGRESS: at 95.86% examples, 637235 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:37,716 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-10 08:12:37,719 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-10 08:12:37,724 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-10 08:12:37,728 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-10 08:12:37,729 : INFO : EPOCH - 4 : training on 17798270 raw words (12751718 effective words) took 20.0s, 637306 effective words/s\n",
      "2018-10-10 08:12:38,744 : INFO : EPOCH 5 - PROGRESS: at 4.70% examples, 596478 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:39,752 : INFO : EPOCH 5 - PROGRESS: at 9.69% examples, 610528 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:40,753 : INFO : EPOCH 5 - PROGRESS: at 14.57% examples, 612025 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 08:12:41,760 : INFO : EPOCH 5 - PROGRESS: at 19.44% examples, 611591 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:42,767 : INFO : EPOCH 5 - PROGRESS: at 24.44% examples, 615721 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:43,779 : INFO : EPOCH 5 - PROGRESS: at 29.09% examples, 610746 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 08:12:44,788 : INFO : EPOCH 5 - PROGRESS: at 34.54% examples, 620654 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:45,799 : INFO : EPOCH 5 - PROGRESS: at 39.31% examples, 618253 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:46,810 : INFO : EPOCH 5 - PROGRESS: at 44.09% examples, 617105 words/s, in_qsize 8, out_qsize 1\n",
      "2018-10-10 08:12:47,817 : INFO : EPOCH 5 - PROGRESS: at 48.87% examples, 616461 words/s, in_qsize 8, out_qsize 0\n",
      "2018-10-10 08:12:48,848 : INFO : EPOCH 5 - PROGRESS: at 53.67% examples, 614023 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:49,864 : INFO : EPOCH 5 - PROGRESS: at 58.51% examples, 614362 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:50,895 : INFO : EPOCH 5 - PROGRESS: at 63.45% examples, 614039 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:51,919 : INFO : EPOCH 5 - PROGRESS: at 68.32% examples, 613628 words/s, in_qsize 8, out_qsize 1\n",
      "2018-10-10 08:12:52,935 : INFO : EPOCH 5 - PROGRESS: at 73.06% examples, 612612 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:53,937 : INFO : EPOCH 5 - PROGRESS: at 77.77% examples, 611762 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:54,947 : INFO : EPOCH 5 - PROGRESS: at 82.72% examples, 612464 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:55,956 : INFO : EPOCH 5 - PROGRESS: at 87.59% examples, 612748 words/s, in_qsize 6, out_qsize 1\n",
      "2018-10-10 08:12:56,962 : INFO : EPOCH 5 - PROGRESS: at 92.50% examples, 613414 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:57,989 : INFO : EPOCH 5 - PROGRESS: at 97.35% examples, 612676 words/s, in_qsize 7, out_qsize 0\n",
      "2018-10-10 08:12:58,494 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-10-10 08:12:58,499 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-10-10 08:12:58,513 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-10-10 08:12:58,526 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-10-10 08:12:58,528 : INFO : EPOCH - 5 : training on 17798270 raw words (12750831 effective words) took 20.8s, 613291 effective words/s\n",
      "2018-10-10 08:12:58,529 : INFO : training on a 88991350 raw words (63757691 effective words) took 102.6s, 621140 effective words/s\n",
      "2018-10-10 08:12:58,530 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-10-10 08:12:58,696 : INFO : saving Word2Vec object under W2V_MODEL, separately None\n",
      "2018-10-10 08:12:58,698 : INFO : not storing attribute vectors_norm\n",
      "2018-10-10 08:12:58,700 : INFO : not storing attribute cum_table\n",
      "2018-10-10 08:12:59,293 : INFO : saved W2V_MODEL\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"W2V_MODEL\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"W2V_MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to average all of the word vectors in a given paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.true_divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the average feature vector for one and returns a 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews,model,num_features):\n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "        if counter%1000. == 0.:\n",
    "            print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate average feature vectors for training and testing sets,using the functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guru_dsce11/.local/lib/python3.5/site-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review 0 of 25000\n",
      "Review 1000 of 25000\n",
      "Review 2000 of 25000\n",
      "Review 3000 of 25000\n",
      "Review 4000 of 25000\n",
      "Review 5000 of 25000\n",
      "Review 6000 of 25000\n",
      "Review 7000 of 25000\n",
      "Review 8000 of 25000\n",
      "Review 9000 of 25000\n",
      "Review 10000 of 25000\n",
      "Review 11000 of 25000\n",
      "Review 12000 of 25000\n",
      "Review 13000 of 25000\n",
      "Review 14000 of 25000\n",
      "Review 15000 of 25000\n",
      "Review 16000 of 25000\n",
      "Review 17000 of 25000\n",
      "Review 18000 of 25000\n",
      "Review 19000 of 25000\n",
      "Review 20000 of 25000\n",
      "Review 21000 of 25000\n",
      "Review 22000 of 25000\n",
      "Review 23000 of 25000\n",
      "Review 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review,remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print(\"Creating average feature vecs for test reviews\")\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review,remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews,model,num_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once the vectors are ready it has to be trained .Here MLPs are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The Sequential model is a linear stack of layers.\n",
    "+ It can be created by passing a list of layer instances to the constructor:\n",
    "+ The layers can be created via .add() method\n",
    "+ Dense implements the operation: output = activation(dot(input, kernel) + bias) where\n",
    "+ activation is the element-wise activation function passed as the activation argument,there are many activation functions ar available ex: tanh, relu, softmax \n",
    "+ kernel is a weights matrix created by the layer, and \n",
    "+ bias is a bias vector created by the layer (only applicable if use_bias is True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 233,089\n",
      "Trainable params: 231,617\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, activation='relu',input_shape=(300,),kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu',input_shape=(300,),kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu',input_shape=(300,),kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(32, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.save(\"MLP_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 7s 261us/step - loss: 0.5137 - acc: 0.7631\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 3s 131us/step - loss: 0.3764 - acc: 0.8422\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 3s 132us/step - loss: 0.3588 - acc: 0.8513\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 3s 131us/step - loss: 0.3447 - acc: 0.8572\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 3s 128us/step - loss: 0.3467 - acc: 0.8562\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 3s 129us/step - loss: 0.3352 - acc: 0.8617\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 3s 128us/step - loss: 0.3388 - acc: 0.8582\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 3s 129us/step - loss: 0.3324 - acc: 0.8606\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 3s 130us/step - loss: 0.3303 - acc: 0.8620\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 3s 131us/step - loss: 0.3274 - acc: 0.8633\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 3s 128us/step - loss: 0.3267 - acc: 0.8626\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 3s 130us/step - loss: 0.3255 - acc: 0.8639\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 3s 130us/step - loss: 0.3216 - acc: 0.8654\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 3s 129us/step - loss: 0.3195 - acc: 0.8647\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 3s 130us/step - loss: 0.3181 - acc: 0.8675\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 3s 130us/step - loss: 0.3206 - acc: 0.8661\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 3s 128us/step - loss: 0.3148 - acc: 0.8695\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 3s 128us/step - loss: 0.3139 - acc: 0.8711\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 3s 129us/step - loss: 0.3141 - acc: 0.8685\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 3s 131us/step - loss: 0.3095 - acc: 0.8709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7e5d3a2fd0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit( trainDataVecs, train[\"sentiment\"],epochs=20,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(result)):\n",
    "    if(result[i]>0.5):\n",
    "        result[i]=1\n",
    "    else:\n",
    "        result[i]=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=result.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=result.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"output.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
